diff --git a/drivers/staging/android/ion/ion_priv.h b/drivers/staging/android/ion/ion_priv.h
index 0e3b8a6f3bf7..52b430151590 100644
--- a/drivers/staging/android/ion/ion_priv.h
+++ b/drivers/staging/android/ion/ion_priv.h
@@ -26,14 +26,43 @@
 #include <linux/sched.h>
 #include <linux/shrinker.h>
 #include <linux/types.h>
+#include <linux/semaphore.h>
+#include <linux/vmalloc.h>
+#include <linux/dma-direction.h>
+#include <trace/events/ion.h>
+#include <asm/cacheflush.h>
+
 #ifdef CONFIG_ION_POOL_CACHE_POLICY
 #include <asm/cacheflush.h>
 #endif
 
 #include "ion.h"
 
+struct exynos_ion_platform_heap {
+	struct ion_platform_heap heap_data;
+	struct reserved_mem *rmem;
+	unsigned int id;
+	unsigned int compat_ids;
+	bool secure;
+	bool reusable;
+	bool protected;
+	bool noprot;
+	bool should_isolate;
+	atomic_t secure_ref;
+	struct device dev;
+	struct ion_heap *heap;
+};
+
 struct ion_buffer *ion_handle_buffer(struct ion_handle *handle);
 
+struct ion_iovm_map {
+	struct list_head list;
+	unsigned int map_cnt;
+	struct device *dev;
+	struct iommu_domain *domain;
+	dma_addr_t iova;
+};
+
 /**
  * struct ion_buffer - metadata for a particular buffer
  * @ref:		refernce count
@@ -83,11 +112,27 @@ struct ion_buffer {
 	struct sg_table *sg_table;
 	struct page **pages;
 	struct list_head vmas;
+	struct list_head iovas;
 	/* used to track orphaned buffers */
 	int handle_count;
 	char task_comm[TASK_COMM_LEN];
 	pid_t pid;
+
+#ifdef CONFIG_ION_EXYNOS_STAT_LOG
+	struct list_head master_list;
+	char thread_comm[TASK_COMM_LEN];
+	pid_t tid;
+#endif
+};
+
+#ifdef CONFIG_ION_EXYNOS_STAT_LOG
+struct ion_task {
+	struct list_head list;
+	struct kref ref;
+	struct device *master;
 };
+#endif
+
 void ion_buffer_destroy(struct ion_buffer *buffer);
 
 /**
@@ -124,8 +169,13 @@ struct ion_heap_ops {
 	int (*map_user)(struct ion_heap *mapper, struct ion_buffer *buffer,
 			struct vm_area_struct *vma);
 	int (*shrink)(struct ion_heap *heap, gfp_t gfp_mask, int nr_to_scan);
+	void (*preload) (struct ion_heap *heap, unsigned int count,
+			 unsigned int flags, struct ion_preload_object obj[]);
 };
 
+/* [INTERNAL USE ONLY] threshold value for whole cache flush */
+#define ION_FLUSH_ALL_HIGHLIMIT SZ_8M
+
 /**
  * heap flags - flags between the heaps and core ion code
  */
@@ -142,6 +192,13 @@ struct ion_heap_ops {
  */
 #define ION_PRIV_FLAG_SHRINKER_FREE (1 << 0)
 
+/*
+ * Following private flags are used for deferred init to boost up
+ * allocation performance.
+ * READY_TO_USE: set when buffer is initialized
+ */
+#define ION_PRIV_FLAG_READY_TO_USE (1 << 15)
+
 /**
  * struct ion_heap - represents a heap in the system
  * @node:		rb node to put the heap on the device's tree of heaps
@@ -159,6 +216,10 @@ struct ion_heap_ops {
  * @lock:		protects the free list
  * @waitqueue:		queue to wait on from deferred free thread
  * @task:		task struct of deferred free thread
+ * @vm_sem:		semaphore for reserved_vm_area
+ * @page_idx:		index of reserved_vm_area slots
+ * @reserved_vm_area:	reserved vm area
+ * @pte:		pte lists for reserved_vm_area
  * @debug_show:		called when heap debug file is read to add any
  *			heap specific debug info to output
  *
@@ -183,15 +244,32 @@ struct ion_heap {
 	struct task_struct *task;
 
 	int (*debug_show)(struct ion_heap *heap, struct seq_file *, void *);
+	atomic_long_t total_allocated;
+	atomic_long_t total_allocated_peak;
+	atomic_long_t total_handles;
 };
 
+/**
+ * ion_buffer_sync_force - check if ION_FLAG_SYNC_FORCE is set
+ * @buffer:		buffer
+ *
+ * indicates whether this ion buffer should be cache clean after allocation
+ */
+static inline bool ion_buffer_sync_force(struct ion_buffer *buffer)
+{
+	return !!(buffer->flags & ION_FLAG_SYNC_FORCE);
+}
+
 /**
  * ion_buffer_cached - this ion buffer is cached
  * @buffer:		buffer
  *
  * indicates whether this ion buffer is cached
  */
-bool ion_buffer_cached(struct ion_buffer *buffer);
+static inline bool ion_buffer_cached(struct ion_buffer *buffer)
+{
+	return !!(buffer->flags & ION_FLAG_CACHED);
+}
 
 /**
  * ion_buffer_fault_user_mappings - fault in user mappings of this buffer
@@ -200,7 +278,16 @@ bool ion_buffer_cached(struct ion_buffer *buffer);
  * indicates whether userspace mappings of this buffer will be faulted
  * in, this can affect how buffers are allocated from the heap.
  */
-bool ion_buffer_fault_user_mappings(struct ion_buffer *buffer);
+static inline bool ion_buffer_fault_user_mappings(struct ion_buffer *buffer)
+{
+	return (buffer->flags & ION_FLAG_CACHED) &&
+		!(buffer->flags & ION_FLAG_CACHED_NEEDS_SYNC);
+}
+
+static inline bool ion_buffer_need_flush_all(struct ion_buffer *buffer)
+{
+	return buffer->size >= ION_FLUSH_ALL_HIGHLIMIT;
+}
 
 /**
  * ion_device_create - allocates and returns an ion device
@@ -325,11 +412,54 @@ void ion_system_contig_heap_destroy(struct ion_heap *);
 struct ion_heap *ion_carveout_heap_create(struct ion_platform_heap *);
 void ion_carveout_heap_destroy(struct ion_heap *);
 
+void ion_debug_heap_usage_show(struct ion_heap *heap);
 struct ion_heap *ion_chunk_heap_create(struct ion_platform_heap *);
 void ion_chunk_heap_destroy(struct ion_heap *);
 struct ion_heap *ion_cma_heap_create(struct ion_platform_heap *);
 void ion_cma_heap_destroy(struct ion_heap *);
 
+typedef void (*ion_device_sync_func)(const void *, size_t, int);
+void ion_device_sync(struct ion_device *dev, struct scatterlist *sgl,
+			int nents, enum dma_data_direction dir,
+			ion_device_sync_func sync, bool memzero);
+
+#ifdef CONFIG_HIGHMEM
+static inline void ion_buffer_set_ready(struct ion_buffer *buffer)
+{
+	buffer->private_flags |= ION_PRIV_FLAG_READY_TO_USE;
+}
+
+static inline void ion_buffer_flush(const void *vaddr, size_t size, int dir)
+{
+	dmac_flush_range(vaddr, vaddr + size);
+}
+
+static inline void ion_buffer_make_ready(struct ion_buffer *buffer)
+{
+	if (!(buffer->private_flags & ION_PRIV_FLAG_READY_TO_USE)) {
+		ion_device_sync(buffer->dev, buffer->sg_table->sgl,
+			buffer->sg_table->nents, DMA_BIDIRECTIONAL,
+			(ion_buffer_cached(buffer) &&
+				 !ion_buffer_fault_user_mappings(buffer)) ?
+				NULL : ion_buffer_flush,
+			!(buffer->flags & ION_FLAG_NOZEROED));
+		buffer->private_flags |= ION_PRIV_FLAG_READY_TO_USE;
+	}
+}
+
+static inline void ion_buffer_make_ready_lock(struct ion_buffer *buffer)
+{
+	mutex_lock(&buffer->lock);
+	ion_buffer_make_ready(buffer);
+	mutex_unlock(&buffer->lock);
+}
+#else
+#define ion_buffer_flush NULL
+#define ion_buffer_set_ready(buffer) do { } while (0)
+#define ion_buffer_make_ready(buffer) do { } while (0)
+#define ion_buffer_make_ready_lock(buffer) do { } while (0)
+#endif
+
 /**
  * kernel api to allocate/free from carveout -- used when carveout is
  * used to back an architecture specific custom heap
@@ -351,6 +481,17 @@ void ion_carveout_free(struct ion_heap *heap, ion_phys_addr_t addr,
  * invalidated from the cache, provides a significant performance benefit on
  * many systems */
 
+/**
+ * There are 2 separated page pools, i.e. cached and noncached.
+ * All pages in the noncached pool have the page flag 'PG_ion_frompool' by
+ * ion_page_pool_add() because only noncached pages should be sorted out at
+ * the allocation time. The 'PG_ion_frompool' is removed just before calling
+ * __free_page(s) to return to kernel.
+ */
+#define ion_set_page_clean(page)	set_bit(PG_dcache_clean, &(page)->flags)
+#define ion_get_page_clean(page)	test_bit(PG_dcache_clean, &(page)->flags)
+#define ion_clear_page_clean(page)	clear_bit(PG_dcache_clean, &(page)->flags)
+
 /**
  * struct ion_page_pool - pagepool struct
  * @high_count:		number of highmem items in the pool
@@ -373,14 +514,16 @@ struct ion_page_pool {
 	int low_count;
 	struct list_head high_items;
 	struct list_head low_items;
-	struct mutex mutex;
+	spinlock_t lock;
 	gfp_t gfp_mask;
 	unsigned int order;
+	bool cached;
 	struct plist_node list;
 };
 
 struct ion_page_pool *ion_page_pool_create(gfp_t gfp_mask, unsigned int order);
 void ion_page_pool_destroy(struct ion_page_pool *);
+void *ion_page_pool_alloc_pages(struct ion_page_pool *pool);
 struct page *ion_page_pool_alloc(struct ion_page_pool *);
 void ion_page_pool_free(struct ion_page_pool *, struct page *);
 void ion_page_pool_free_immediate(struct ion_page_pool *, struct page *);
@@ -436,4 +579,80 @@ int ion_page_pool_shrink(struct ion_page_pool *pool, gfp_t gfp_mask,
 void ion_pages_sync_for_device(struct device *dev, struct page *page,
 		size_t size, enum dma_data_direction dir);
 
+void ion_page_pool_preload_prepare(struct ion_page_pool *pool, long num_pages);
+long ion_page_pool_preload(struct ion_page_pool *pool,
+			   struct ion_page_pool *alt_pool,
+			   unsigned int alloc_flags, long num_pages);
+
+#ifdef CONFIG_ION_EXYNOS_STAT_LOG
+#define ION_EVENT_LOG_MAX	1024
+#define ION_EVENT_BEGIN()	ktime_t begin = ktime_get()
+#define ION_EVENT_DONE()	begin
+
+typedef enum ion_event_type {
+	ION_EVENT_TYPE_ALLOC = 0,
+	ION_EVENT_TYPE_FREE,
+	ION_EVENT_TYPE_MMAP,
+	ION_EVENT_TYPE_SHRINK,
+	ION_EVENT_TYPE_CLEAR,
+} ion_event_t;
+
+#define ION_EVENT_HEAPNAME	8
+struct ion_event_alloc {
+	void *id;
+	unsigned char heapname[ION_EVENT_HEAPNAME];
+	size_t size;
+	unsigned long flags;
+};
+
+struct ion_event_free {
+	void *id;
+	unsigned char heapname[ION_EVENT_HEAPNAME];
+	size_t size;
+	bool shrinker;
+};
+
+struct ion_event_mmap {
+	void *id;
+	unsigned char heapname[ION_EVENT_HEAPNAME];
+	size_t size;
+};
+
+struct ion_event_shrink {
+	size_t size;
+};
+
+struct ion_event_clear {
+	void *id;
+	unsigned char heapname[ION_EVENT_HEAPNAME];
+	size_t size;
+	unsigned long flags;
+};
+
+struct ion_eventlog {
+	ion_event_t type;
+	union {
+		struct ion_event_alloc alloc;
+		struct ion_event_free free;
+		struct ion_event_mmap mmap;
+		struct ion_event_shrink shrink;
+		struct ion_event_clear clear;
+	} data;
+	ktime_t begin;
+	ktime_t done;
+};
+
+void ION_EVENT_SHRINK(struct ion_device *dev, size_t size);
+void ION_EVENT_CLEAR(struct ion_buffer *buffer, ktime_t begin);
+
+void show_ion_system_heap_size(struct seq_file *s);
+void show_ion_system_heap_pool_size(struct seq_file *s);
+
+#else
+#define ION_EVENT_BEGIN()		do { } while (0)
+#define ION_EVENT_DONE()		do { } while (0)
+#define ION_EVENT_SHRINK(dev, size)	do { } while (0)
+#define ION_EVENT_CLEAR(buffer, begin)	do { } while (0)
+#endif
+
 #endif /* _ION_PRIV_H */
