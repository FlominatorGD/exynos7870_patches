diff --git a/drivers/staging/android/ion/ion_carveout_heap.c b/drivers/staging/android/ion/ion_carveout_heap.c
old mode 100755
new mode 100644
index e702ce6461fc..feb3cee75c1c
--- a/drivers/staging/android/ion/ion_carveout_heap.c
+++ b/drivers/staging/android/ion/ion_carveout_heap.c
@@ -14,30 +14,57 @@
  *
  */
 #include <linux/spinlock.h>
+#include <linux/bitmap.h>
 #include <linux/dma-mapping.h>
 #include <linux/err.h>
 #include <linux/genalloc.h>
 #include <linux/io.h>
+#include <linux/ion.h>
 #include <linux/mm.h>
 #include <linux/scatterlist.h>
 #include <linux/slab.h>
 #include <linux/vmalloc.h>
-#include "ion.h"
+#include <linux/exynos_ion.h>
 #include "ion_priv.h"
 
+#define MAX_CARVEOUT_ALIGNMENT	12
+
+extern struct ion_device *g_idev;
+
 struct ion_carveout_heap {
 	struct ion_heap heap;
 	struct gen_pool *pool;
 	ion_phys_addr_t base;
 };
 
+static unsigned long find_first_fit_with_align(unsigned long *map,
+				unsigned long size, unsigned long start,
+				unsigned int nr, void *data)
+{
+	unsigned long align = ((*(unsigned long *)data) >> PAGE_SHIFT);
+
+	if (align > (1 << MAX_CARVEOUT_ALIGNMENT))
+		align = (1 << MAX_CARVEOUT_ALIGNMENT);
+
+	return bitmap_find_next_zero_area(map, size, start, nr, (align - 1));
+}
+
 ion_phys_addr_t ion_carveout_allocate(struct ion_heap *heap,
 				      unsigned long size,
 				      unsigned long align)
 {
 	struct ion_carveout_heap *carveout_heap =
 		container_of(heap, struct ion_carveout_heap, heap);
-	unsigned long offset = gen_pool_alloc(carveout_heap->pool, size);
+	unsigned long offset;
+
+	if (align > PAGE_SIZE) {
+		gen_pool_set_algo(carveout_heap->pool,
+				find_first_fit_with_align, &align);
+		offset = gen_pool_alloc(carveout_heap->pool, size);
+		gen_pool_set_algo(carveout_heap->pool, NULL, NULL);
+	} else {
+		offset = gen_pool_alloc(carveout_heap->pool, size);
+	}
 
 	if (!offset)
 		return ION_CARVEOUT_ALLOCATE_FAIL;
@@ -74,13 +101,18 @@ static int ion_carveout_heap_allocate(struct ion_heap *heap,
 				      unsigned long size, unsigned long align,
 				      unsigned long flags)
 {
+	struct ion_carveout_heap *carveout_heap =
+		container_of(heap, struct ion_carveout_heap, heap);
 	struct sg_table *table;
 	ion_phys_addr_t paddr;
 	int ret;
 
-	if (align > PAGE_SIZE)
+	if ((align > PAGE_SIZE) && (align & (align - 1)))
 		return -EINVAL;
 
+	if (!ion_is_heap_available(heap, flags, carveout_heap->pool))
+		return -EPERM;
+
 	table = kmalloc(sizeof(struct sg_table), GFP_KERNEL);
 	if (!table)
 		return -ENOMEM;
@@ -97,12 +129,19 @@ static int ion_carveout_heap_allocate(struct ion_heap *heap,
 	sg_set_page(table->sgl, pfn_to_page(PFN_DOWN(paddr)), size, 0);
 	buffer->priv_virt = table;
 
+	if (buffer->flags & ION_FLAG_PROTECTED) {
+		ret = ion_secure_protect(buffer);
+		if (ret)
+			goto err_free_table;
+	}
+
 	return 0;
 
 err_free_table:
 	sg_free_table(table);
 err_free:
 	kfree(table);
+	ion_debug_heap_usage_show(heap);
 	return ret;
 }
 
@@ -110,14 +149,23 @@ static void ion_carveout_heap_free(struct ion_buffer *buffer)
 {
 	struct ion_heap *heap = buffer->heap;
 	struct sg_table *table = buffer->priv_virt;
-	struct page *page = sg_page(table->sgl);
-	ion_phys_addr_t paddr = PFN_PHYS(page_to_pfn(page));
-
-	ion_heap_buffer_zero(buffer);
-
-	if (ion_buffer_cached(buffer))
-		dma_sync_sg_for_device(NULL, table->sgl, table->nents,
-							DMA_BIDIRECTIONAL);
+	ion_phys_addr_t paddr = PFN_PHYS(page_to_pfn(sg_page(table->sgl)));
+
+	if (!(buffer->flags & ION_FLAG_PROTECTED)) {
+		void *va = page_address(sg_page(table->sgl));
+
+		if (ion_buffer_cached(buffer)) {
+			memset(va, 0, buffer->size);
+			if (ion_buffer_need_flush_all(buffer))
+				flush_all_cpu_caches();
+			else
+				__flush_dcache_area(va, buffer->size);
+		} else {
+			ion_heap_buffer_zero(buffer);
+		}
+	} else {
+		ion_secure_unprotect(buffer);
+	}
 
 	ion_carveout_free(heap, paddr, buffer->size);
 	sg_free_table(table);
@@ -135,6 +183,23 @@ static void ion_carveout_heap_unmap_dma(struct ion_heap *heap,
 {
 }
 
+static int carveout_heap_debug_show(struct ion_heap *heap,
+					struct seq_file *s,
+					void *unused)
+{
+	struct ion_carveout_heap *carveout_heap =
+		container_of(heap, struct ion_carveout_heap, heap);
+
+	seq_puts(s, "\ncarveout heap allocations\n");
+	seq_printf(s, "%11.s %10.s %10.s\n", "name", "size", "free");
+	seq_puts(s, "----------------------------------------------------\n");
+	seq_printf(s, "%11.s %#10.zx %#10.zx\n",
+			heap->name, gen_pool_size(carveout_heap->pool),
+			gen_pool_avail(carveout_heap->pool));
+
+	return 0;
+}
+
 static struct ion_heap_ops carveout_heap_ops = {
 	.allocate = ion_carveout_heap_allocate,
 	.free = ion_carveout_heap_free,
@@ -149,7 +214,6 @@ static struct ion_heap_ops carveout_heap_ops = {
 struct ion_heap *ion_carveout_heap_create(struct ion_platform_heap *heap_data)
 {
 	struct ion_carveout_heap *carveout_heap;
-	int ret;
 
 	struct page *page;
 	size_t size;
@@ -157,12 +221,6 @@ struct ion_heap *ion_carveout_heap_create(struct ion_platform_heap *heap_data)
 	page = pfn_to_page(PFN_DOWN(heap_data->base));
 	size = heap_data->size;
 
-	ion_pages_sync_for_device(NULL, page, size, DMA_BIDIRECTIONAL);
-
-	ret = ion_heap_pages_zero(page, size, pgprot_writecombine(PAGE_KERNEL));
-	if (ret)
-		return ERR_PTR(ret);
-
 	carveout_heap = kzalloc(sizeof(struct ion_carveout_heap), GFP_KERNEL);
 	if (!carveout_heap)
 		return ERR_PTR(-ENOMEM);
@@ -177,8 +235,24 @@ struct ion_heap *ion_carveout_heap_create(struct ion_platform_heap *heap_data)
 		     -1);
 	carveout_heap->heap.ops = &carveout_heap_ops;
 	carveout_heap->heap.type = ION_HEAP_TYPE_CARVEOUT;
-	carveout_heap->heap.flags = ION_HEAP_FLAG_DEFER_FREE;
-
+	carveout_heap->heap.debug_show = carveout_heap_debug_show;
+
+#ifdef CONFIG_ARM64
+	if (size >= ION_FLUSH_ALL_HIGHLIMIT)
+		flush_all_cpu_caches();
+	else
+		__flush_dcache_area(page_address(page), size);
+#else
+	if (size >= ION_FLUSH_ALL_HIGHLIMIT) {
+		flush_all_cpu_caches();
+	} else {
+		struct scatterlist sg;
+
+		sg_set_page(&sg, page, size, 0);
+		ion_device_sync(g_idev, &sg, 1, DMA_BIDIRECTIONAL,
+				ion_buffer_flush, false);
+	}
+#endif
 	return &carveout_heap->heap;
 }
 
