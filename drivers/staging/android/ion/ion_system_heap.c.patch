diff --git a/drivers/staging/android/ion/ion_system_heap.c b/drivers/staging/android/ion/ion_system_heap.c
index 9adfd6de8f8c..b100d9b57950 100644
--- a/drivers/staging/android/ion/ion_system_heap.c
+++ b/drivers/staging/android/ion/ion_system_heap.c
@@ -14,6 +14,7 @@
  *
  */
 
+#include <asm/compat.h>
 #include <asm/page.h>
 #include <linux/dma-mapping.h>
 #include <linux/err.h>
@@ -23,14 +24,25 @@
 #include <linux/seq_file.h>
 #include <linux/slab.h>
 #include <linux/vmalloc.h>
+#include <linux/kthread.h>
+#include <asm/tlbflush.h>
 #include "ion.h"
 #include "ion_priv.h"
 
+#ifdef CONFIG_HIGHMEM
+static gfp_t high_order_gfp_flags = (GFP_HIGHUSER | __GFP_NOWARN |
+				    __GFP_NORETRY | __GFP_NO_KSWAPD) & ~__GFP_WAIT;
+static gfp_t low_order_gfp_flags  = (GFP_HIGHUSER | __GFP_NOWARN);
+#else
 static gfp_t high_order_gfp_flags = (GFP_HIGHUSER | __GFP_ZERO | __GFP_NOWARN |
-				     __GFP_NORETRY) & ~__GFP_WAIT;
+				    __GFP_NORETRY | __GFP_NO_KSWAPD) & ~__GFP_WAIT;
 static gfp_t low_order_gfp_flags  = (GFP_HIGHUSER | __GFP_ZERO | __GFP_NOWARN);
+#endif
+
 static const unsigned int orders[] = {8, 4, 0};
 static const int num_orders = ARRAY_SIZE(orders);
+static struct ion_system_heap *system_heap;
+
 static int order_to_index(unsigned int order)
 {
 	int i;
@@ -49,31 +61,36 @@ static inline unsigned int order_to_size(int order)
 
 struct ion_system_heap {
 	struct ion_heap heap;
-	struct ion_page_pool *pools[0];
+	struct ion_page_pool **pools;
 };
 
 static struct page *alloc_buffer_page(struct ion_system_heap *heap,
 				      struct ion_buffer *buffer,
 				      unsigned long order)
 {
-	bool cached = ion_buffer_cached(buffer);
-	struct ion_page_pool *pool = heap->pools[order_to_index(order)];
+	int idx = order_to_index(order);
+	struct ion_page_pool *pool;
 	struct page *page;
 
-	if (!cached) {
-		page = ion_page_pool_alloc(pool);
-	} else {
-		gfp_t gfp_flags = low_order_gfp_flags;
+	if (!ion_buffer_cached(buffer))
+		idx += num_orders;
 
-		if (order > 4)
-			gfp_flags = high_order_gfp_flags;
-		page = alloc_pages(gfp_flags | __GFP_COMP, order);
-		if (!page)
-			return NULL;
-		ion_pages_sync_for_device(NULL, page, PAGE_SIZE << order,
-						DMA_BIDIRECTIONAL);
+	pool = heap->pools[idx];
+
+	page = ion_page_pool_alloc(pool);
+	if (!page) {
+		/* try with alternative pool */
+		if (ion_buffer_cached(buffer))
+			pool = heap->pools[idx + num_orders];
+		else
+			pool = heap->pools[idx - num_orders];
+
+		page = ion_page_pool_alloc(pool);
 	}
 
+	if (!page)
+		page = ion_page_pool_alloc_pages(pool);
+
 	return page;
 }
 
@@ -81,15 +98,15 @@ static void free_buffer_page(struct ion_system_heap *heap,
 			     struct ion_buffer *buffer, struct page *page)
 {
 	unsigned int order = compound_order(page);
-	bool cached = ion_buffer_cached(buffer);
 
-	if (!cached) {
-		struct ion_page_pool *pool = heap->pools[order_to_index(order)];
-		if (buffer->private_flags & ION_PRIV_FLAG_SHRINKER_FREE)
-			ion_page_pool_free_immediate(pool, page);
-		else
-			ion_page_pool_free(pool, page);
+	if (!(buffer->private_flags & ION_PRIV_FLAG_SHRINKER_FREE)) {
+		int uncached = ion_buffer_cached(buffer) ? 0 : 1;
+		int idx = order_to_index(order) + (num_orders * uncached);
+		struct ion_page_pool *pool = heap->pools[idx];
+
+		ion_page_pool_free(pool, page);
 	} else {
+		ion_clear_page_clean(page);
 		__free_pages(page, order);
 	}
 }
@@ -119,6 +136,9 @@ static struct page *alloc_largest_available(struct ion_system_heap *heap,
 	return NULL;
 }
 
+#define should_flush_cache(page, buffer) (!ion_get_page_clean(page) &&	\
+		(!ion_buffer_cached(buffer) || ion_buffer_sync_force(buffer)))
+
 static int ion_system_heap_allocate(struct ion_heap *heap,
 				     struct ion_buffer *buffer,
 				     unsigned long size, unsigned long align,
@@ -134,6 +154,7 @@ static int ion_system_heap_allocate(struct ion_heap *heap,
 	int i = 0;
 	unsigned long size_remaining = PAGE_ALIGN(size);
 	unsigned int max_order = orders[0];
+	bool all_pages_from_pool = true;
 
 	if (align > PAGE_SIZE)
 		return -EINVAL;
@@ -161,19 +182,35 @@ static int ion_system_heap_allocate(struct ion_heap *heap,
 
 	sg = table->sgl;
 	list_for_each_entry_safe(page, tmp_page, &pages, lru) {
-		sg_set_page(sg, page, PAGE_SIZE << compound_order(page), 0);
+		unsigned int len = PAGE_SIZE << compound_order(page);
+		sg_set_page(sg, page, len, 0);
 		sg = sg_next(sg);
+		if (should_flush_cache(page, buffer)) {
+			all_pages_from_pool = false;
+			if (!IS_ENABLED(CONFIG_HIGHMEM)) {
+				__flush_dcache_area(page_address(page), len);
+				if (!ion_buffer_cached(buffer))
+					ion_set_page_clean(page);
+			}
+		}
 		list_del(&page->lru);
 	}
 
+	if (all_pages_from_pool)
+		ion_buffer_set_ready(buffer);
+
 	buffer->priv_virt = table;
 	return 0;
 
 free_table:
 	kfree(table);
 free_pages:
-	list_for_each_entry_safe(page, tmp_page, &pages, lru)
+	list_for_each_entry_safe(page, tmp_page, &pages, lru) {
+		list_del(&page->lru);
+		buffer->private_flags |= ION_PRIV_FLAG_SHRINKER_FREE;
 		free_buffer_page(sys_heap, buffer, page);
+	}
+
 	return -ENOMEM;
 }
 
@@ -183,13 +220,12 @@ static void ion_system_heap_free(struct ion_buffer *buffer)
 							struct ion_system_heap,
 							heap);
 	struct sg_table *table = buffer->sg_table;
-	bool cached = ion_buffer_cached(buffer);
 	struct scatterlist *sg;
 	int i;
 
-	/* uncached pages come from the page pools, zero them before returning
+	/* pages come from the page pools, zero them before returning
 	   for security purposes (other allocations are zerod at alloc time */
-	if (!cached && !(buffer->private_flags & ION_PRIV_FLAG_SHRINKER_FREE))
+	if (!(buffer->private_flags & ION_PRIV_FLAG_SHRINKER_FREE))
 		ion_heap_buffer_zero(buffer);
 
 	for_each_sg(table->sgl, sg, table->nents, i)
@@ -209,19 +245,191 @@ static void ion_system_heap_unmap_dma(struct ion_heap *heap,
 {
 }
 
+struct ion_system_heap_prealod_object {
+	size_t len;
+	unsigned int count;
+};
+
+struct ion_system_heap_preload_data {
+	struct ion_system_heap *heap;
+	unsigned int flags;
+	unsigned int count;
+	struct ion_system_heap_prealod_object objs[0];
+};
+
+#define ION_FLAG_CACHED_POOL (ION_FLAG_CACHED | ION_FLAG_CACHED_NEEDS_SYNC)
+static int ion_system_heap_preloader(void *p)
+{
+	struct ion_system_heap_preload_data *data = p;
+	long num_1m = 0, num_64k = 0, num_4k = 0;
+	long loaded_pages;
+	int idx = 0, alt_idx = num_orders;
+	int i;
+
+	for (i = 0; i < data->count; i++) {
+		size_t len = data->objs[i].len / PAGE_SIZE;
+
+		num_4k += (len & 0xF) * data->objs[i].count;
+		len >>= 4;
+		num_64k += (len & 0xF) * data->objs[i].count;
+		len >>= 4;
+		num_1m += len * data->objs[i].count;
+	}
+
+	if ((num_4k + num_64k * 16 + num_1m * 256) > (totalram_pages / 4)) {
+		/* too many pages requested */
+		long max_pages = totalram_pages / 4;
+		long exceeded_pages = num_4k + num_64k * 16 + num_1m * 256;
+		exceeded_pages -= max_pages;
+
+		if (num_4k < exceeded_pages) {
+			num_4k = 0;
+			exceeded_pages -= num_4k;
+		} else {
+			num_4k -= exceeded_pages;
+			exceeded_pages = 0;
+		}
+
+		exceeded_pages /= 16;
+
+		if (num_64k < exceeded_pages) {
+			num_64k = 0;
+			exceeded_pages -= num_64k;
+		} else {
+			num_64k -= exceeded_pages;
+			exceeded_pages = 0;
+		}
+
+		exceeded_pages /= 16;
+
+		if (num_1m < exceeded_pages) {
+			num_1m = 0;
+			exceeded_pages -= num_1m;
+		} else {
+			num_1m -= exceeded_pages;
+			exceeded_pages = 0;
+		}
+	}
+
+	ion_heap_freelist_drain(&data->heap->heap, 0);
+
+	if ((data->flags & ION_FLAG_CACHED_POOL) != ION_FLAG_CACHED_POOL) {
+		idx = num_orders; /* non cached */
+		alt_idx = 0;
+	}
+
+	ion_page_pool_preload_prepare(data->heap->pools[idx + 2], num_4k);
+	/* populates order-0 pages first to invoke page reclamation */
+	loaded_pages = ion_page_pool_preload(data->heap->pools[idx + 2],
+			data->heap->pools[alt_idx + 2], data->flags, num_4k);
+	if (loaded_pages < num_4k)
+		/* kernel is really unable to allocate page */
+		goto finish;
+
+	loaded_pages = ion_page_pool_preload(data->heap->pools[idx + 1],
+			data->heap->pools[alt_idx + 1], data->flags, num_64k);
+	num_64k -= loaded_pages;
+
+	loaded_pages = ion_page_pool_preload(data->heap->pools[idx],
+			data->heap->pools[alt_idx], data->flags, num_1m);
+	num_1m -= loaded_pages;
+
+	if (num_1m || num_64k) {
+		/* try again with lower order free list */
+		loaded_pages = ion_page_pool_preload(data->heap->pools[idx + 1],
+				data->heap->pools[alt_idx + 1], data->flags,
+				num_64k + num_1m * 16);
+		if (num_1m > (loaded_pages / 16)) {
+			num_1m -= loaded_pages / 16;
+			loaded_pages &= 0xF; /* remiander of loaded_pages/16 */
+		} else {
+			loaded_pages -= num_1m * 16;
+			num_1m = 0;
+		}
+		num_64k -= loaded_pages;
+		/*
+		 * half of order-8 pages won't be tried with order-0 free list
+		 * for memory utilization because populating too much low order
+		 * pages causes memory fragmentation seriously.
+		 */
+		num_64k += num_1m * 8;
+		num_4k += num_64k * 16;
+
+		loaded_pages = ion_page_pool_preload(data->heap->pools[idx + 2],
+				data->heap->pools[alt_idx + 2], data->flags,
+				num_4k + num_64k * 16);
+
+		if (((num_4k - loaded_pages) + num_1m) > 0)
+			pr_info("%s: %ld pages are not populated to the pool\n",
+				__func__, loaded_pages + num_1m * 256);
+	}
+
+finish:
+	kfree(data); /* allocated in ion_system_heap_preload_allocate() */
+
+	if (!signal_pending(current))
+		do_exit(0);
+	return 0;
+}
+
+static void ion_system_heap_preload_allocate(struct ion_heap *heap,
+					     unsigned int flags,
+					     unsigned int count,
+					     struct ion_preload_object obj[])
+{
+	struct sched_param param = { .sched_priority = 0 };
+	struct ion_system_heap_preload_data *data;
+	struct task_struct *ret;
+
+	data = kmalloc(sizeof(*data) + sizeof(data->objs[0]) * count,
+			GFP_KERNEL);
+	if (!data) {
+		pr_info("%s: preload request failed due to nomem\n", __func__);
+		return;
+	}
+
+	data->heap = container_of(heap, struct ion_system_heap, heap);
+	data->flags = flags;
+	data->count = count;
+	for (count = 0; count < data->count; count++) {
+		data->objs[count].count = obj[count].count;
+		data->objs[count].len = obj[count].len;
+	}
+
+	ret = kthread_run(ion_system_heap_preloader, data,
+				"ion_system_heap_preloader_%d", count);
+	if (IS_ERR(ret)) {
+		pr_info("%s: failed to create preload thread(%ld)\n",
+				__func__, PTR_ERR(ret));
+	} else {
+		sched_setscheduler(ret, SCHED_NORMAL, &param);
+	}
+}
+
 static int ion_system_heap_shrink(struct ion_heap *heap, gfp_t gfp_mask,
 					int nr_to_scan)
 {
 	struct ion_system_heap *sys_heap;
 	int nr_total = 0;
-	int i;
+	int nocached, i;
 
 	sys_heap = container_of(heap, struct ion_system_heap, heap);
 
-	for (i = 0; i < num_orders; i++) {
-		struct ion_page_pool *pool = sys_heap->pools[i];
-
-		nr_total += ion_page_pool_shrink(pool, gfp_mask, nr_to_scan);
+	/* cached pools first, low order pages first */
+	for (nocached = 0; nocached < 2; nocached++) {
+		for (i = 0; i < num_orders; i++) {
+			struct ion_page_pool *pool =
+				sys_heap->pools[i + num_orders * nocached];
+			int nr_pages = ion_page_pool_shrink(pool, gfp_mask, 0);
+
+			if (nr_to_scan > 0) {
+				int to_scan = nr_to_scan;
+				nr_to_scan -= nr_pages;
+				nr_pages = ion_page_pool_shrink(
+						pool, gfp_mask, to_scan);
+			}
+			nr_total += nr_pages;
+		}
 	}
 
 	return nr_total;
@@ -236,6 +444,7 @@ static struct ion_heap_ops system_heap_ops = {
 	.unmap_kernel = ion_heap_unmap_kernel,
 	.map_user = ion_heap_map_user,
 	.shrink = ion_system_heap_shrink,
+	.preload = ion_system_heap_preload_allocate,
 };
 
 static int ion_system_heap_debug_show(struct ion_heap *heap, struct seq_file *s,
@@ -250,48 +459,113 @@ static int ion_system_heap_debug_show(struct ion_heap *heap, struct seq_file *s,
 	for (i = 0; i < num_orders; i++) {
 		struct ion_page_pool *pool = sys_heap->pools[i];
 
-		seq_printf(s, "%d order %u highmem pages in pool = %lu total\n",
+		seq_printf(s, "%d order %u highmem pages in cached pool = %lu total\n",
+			   pool->high_count, pool->order,
+			   (PAGE_SIZE << pool->order) * pool->high_count);
+		seq_printf(s, "%d order %u lowmem pages in cached pool = %lu total\n",
+			   pool->low_count, pool->order,
+			   (PAGE_SIZE << pool->order) * pool->low_count);
+	}
+
+	for (i = num_orders; i < (num_orders * 2); i++) {
+		struct ion_page_pool *pool = sys_heap->pools[i];
+
+		seq_printf(s, "%d order %u highmem pages in uncached pool = %lu total\n",
 			   pool->high_count, pool->order,
 			   (PAGE_SIZE << pool->order) * pool->high_count);
-		seq_printf(s, "%d order %u lowmem pages in pool = %lu total\n",
+		seq_printf(s, "%d order %u lowmem pages in uncached pool = %lu total\n",
 			   pool->low_count, pool->order,
 			   (PAGE_SIZE << pool->order) * pool->low_count);
 	}
+
 	return 0;
 }
 
+void show_ion_system_heap_size(struct seq_file *s)
+{
+	struct ion_heap *heap;
+	unsigned long system_byte = 0;
+
+	if (!system_heap) {
+		pr_err("system_heap is not ready\n");
+		return;
+	}
+
+	heap = &system_heap->heap;
+	system_byte = (unsigned int)atomic_long_read(&heap->total_allocated);
+	if (s)
+		seq_printf(s, "SystemHeap:     %8lu kB\n", system_byte >> 10);
+	else
+		pr_cont("SystemHeap:%lukB ", system_byte >> 10);
+}
+
+void show_ion_system_heap_pool_size(struct seq_file *s)
+{
+	unsigned long pool_size = 0;
+	struct ion_page_pool *pool;
+	int i;
+
+	if (!system_heap) {
+		pr_err("system_heap_pool is not ready\n");
+		return;
+	}
+
+	for (i = 0; i < num_orders * 2; i++) {
+		pool = system_heap->pools[i];
+		pool_size += (1 << pool->order) * pool->high_count;
+		pool_size += (1 << pool->order) * pool->low_count;
+	}
+
+	if (s)
+		seq_printf(s, "SystemHeapPool: %8lu kB\n",
+			   pool_size << (PAGE_SHIFT - 10));
+	else
+		pr_cont("SystemHeapPool:%lukB ",
+			pool_size << (PAGE_SHIFT - 10));
+}
+
 struct ion_heap *ion_system_heap_create(struct ion_platform_heap *unused)
 {
 	struct ion_system_heap *heap;
 	int i;
 
-	heap = kzalloc(sizeof(struct ion_system_heap) +
-			sizeof(struct ion_page_pool *) * num_orders,
-			GFP_KERNEL);
+	heap = kzalloc(sizeof(struct ion_system_heap), GFP_KERNEL);
 	if (!heap)
 		return ERR_PTR(-ENOMEM);
 	heap->heap.ops = &system_heap_ops;
 	heap->heap.type = ION_HEAP_TYPE_SYSTEM;
 	heap->heap.flags = ION_HEAP_FLAG_DEFER_FREE;
-
-	for (i = 0; i < num_orders; i++) {
+	heap->pools = kzalloc(sizeof(struct ion_page_pool *) * num_orders * 2,
+			      GFP_KERNEL);
+	if (!heap->pools)
+		goto free_heap;
+	for (i = 0; i < num_orders * 2; i++) {
 		struct ion_page_pool *pool;
 		gfp_t gfp_flags = low_order_gfp_flags;
 
-		if (orders[i] > 4)
+		if (orders[i % num_orders] > 0)
 			gfp_flags = high_order_gfp_flags;
-		pool = ion_page_pool_create(gfp_flags, orders[i]);
+		pool = ion_page_pool_create(gfp_flags, orders[i % num_orders]);
 		if (!pool)
 			goto destroy_pools;
+		pool->cached = i < num_orders ? true : false;
 		heap->pools[i] = pool;
 	}
 
 	heap->heap.debug_show = ion_system_heap_debug_show;
+	
+	if (!system_heap)
+		system_heap = heap;
+	else
+		pr_err("system_heap had been already created\n");
+	
 	return &heap->heap;
 
 destroy_pools:
 	while (i--)
 		ion_page_pool_destroy(heap->pools[i]);
+	kfree(heap->pools);
+free_heap:
 	kfree(heap);
 	return ERR_PTR(-ENOMEM);
 }
@@ -303,8 +577,9 @@ void ion_system_heap_destroy(struct ion_heap *heap)
 							heap);
 	int i;
 
-	for (i = 0; i < num_orders; i++)
+	for (i = 0; i < num_orders * 2; i++)
 		ion_page_pool_destroy(sys_heap->pools[i]);
+	kfree(sys_heap->pools);
 	kfree(sys_heap);
 }
 
